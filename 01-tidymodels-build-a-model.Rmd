
# (PART) BUILD A MODEL {-} 

# Introduction {#intro}


```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = TRUE,
                      fig.width = 8, fig.height = 5,
                      eval=TRUE)
library(tidyverse)
library(skimr)
library(GGally)
library(ggmap)
library(tidymodels)

theme_set(theme_classic())
```

*The following content is adapted from the excellent book “Hands-on machine learning with scikit-learn, keras and tensorflow” from @Geron2019.*

In this tutorial you will learn how to specify a regression model with the tidymodels package. To use the code in this article, you will need to install the following packages: 

* [tidyverse](https://www.tidyverse.org/) 
* [tidymodels](https://www.tidymodels.org/) 
* [skimr](https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html) 
* [GGally](https://ggobi.github.io/ggally/index.html)
* [visdat](https://github.com/ropensci/visdat)
* [ggmap](https://github.com/dkahle/ggmap)

```{r}

library(tidyverse)
library(tidymodels)
library(skimr)
library(GGally)
library(visdat)
library(ggmap)

```


In this example, our goal is to build a model of housing prices in California. In particular, the model should learn from California census data and be able to predict the median house price in any district (population of 600 to 3000 people), given some predictor variables. We use the root mean square error (RMSE) as a performance measure for our regression problem.


# Data understanding

:::note
In Data Understanding, you:

- Import data 
- Get an overview about the data structure
- Split data into training and test set using stratified sampling
- Discover and visualize the data to gain insights 
:::

## Imort Data

First of all, let's import the data:

```{r}

LINK <- "https://raw.githubusercontent.com/kirenz/datasets/master/housing.csv"
housing_df <- read_csv(LINK)

```

## Data overview

Next, we take a look at the data structure. California census top 4 rows of the DataFrame: 

```{r}

head(housing_df, 4)

```

Data info:

```{r}

glimpse(housing_df)

```

The package `visdat` helps explore the data class structure:

```{r}

vis_dat(housing_df)

```

Missing data can be arranged by columns with most missingness: 

```{r}

vis_miss(housing_df, sort_miss = TRUE)

```

Alternative method to obtain missing data: 

```{r}
is.na(housing_df) %>% colSums()
```


Note that we have a missing rate of 0.1% (207 cases) in our variable `total_bedroms` which can cause problems for some algorithms. We will take care of this issue in our data preparation. 

## Data splitting

Before we get started with data exploration, let’s split our single dataset into two: a training set and a testing set. 

We’ll keep most of the rows in the original dataset (subset chosen randomly) in the training set. The training data will be used to fit the model, and the testing set will be used to measure model performance.

In our data split, we want to ensure that the training and test set is representative of the various categories of median house values in the whole dataset. Take a look at \@ref(fig:hist-med-value)


```{r hist-med-value, fig.cap="Histogram of Median Proces", out.width='80%'}

housing_df %>% 
  ggplot(aes(median_house_value)) +
  geom_histogram(bins = 4) 

```


In other words, we would like to have instances for each *stratum*, or else the estimate of a stratum's importance may be biased. A *stratum* (plural strata) refers to a subset (part) of the population (entire collection of items under consideration) which is being sampled. You should not have too many strata, and each stratum should be large enough. We use 4 strata in our example. 

To split the data, we can use the `rsample` package (included in `tidymodels`) to create an object that contains the information on how to split the data (which we call `data_split`), and then two more rsample functions to create data frames for the training and testing sets:

```{r}

# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible 
# when random numbers are used 
set.seed(123)

# Put 3/4 of the data into the training set 
data_split <- initial_split(housing_df, 
                           prop = 3/4, 
                           strata = median_house_value, 
                           breaks = 4)

# Create data frames for the two sets:
train_data <- training(data_split) 
test_data <- testing(data_split)

```

## Data exploration

Now let's examine the training data. We first make a copy of the data:

```{r}
data_explore <- train_data
```



We obtain a data summary of numerical and categorical attributes using a function from the package `skimr`:

```{r}

skim(data_explore)

```

Let's count the levels of our categorical variable `ocean_proximity` to get an overview about the variable:

```{r}

data_explore %>% 
  count(ocean_proximity,
        sort = TRUE)

```


The function `ggscatmat` from the package `GGally` creates a matrix with scatterplots, densities and correlations for numeric columns. In our code, we enter the dataset `housing_df`, choose columns 6 to 9, a color column for our categorical variable `ocean_proximity`, and an alpha level of 0.8 (for transparency).

```{r }

ggscatmat(data_explore, columns = 6:9, color="ocean_proximity", alpha=0.8)

```

To obtain an overview of even more visualizations, we can use the function `ggpairs`:

```{r }

ggpairs(data_explore)

```

To plot only correlations of your data, we can use `vis_cor`. We first use Pearson's correlation coefficient:

```{r}

data_explore %>% 
  select(where(is.numeric)) %>% 
  vis_cor(cor_method = "pearson")

```

Spearman's correlation coefficient:

```{r}

data_explore %>% 
  select(where(is.numeric)) %>% 
  vis_cor(cor_method = "spearman")

```


One important thing you may want to do before preparing the data for our algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. Let’s create these new attributes:

```{r}

data_explore <- 
  data_explore %>% 
  mutate(rooms_per_household = total_rooms/households,
        bedrooms_per_room = total_bedrooms/total_rooms,
        population_per_household = population/households)

```

And now let’s look at the correlation matrix again:

```{r}

data_explore %>% 
  select(where(is.numeric)) %>% 
  vis_cor(cor_method = "spearman")

```

Note that the new bedrooms_per_room attribute is  more correlated with the median house value than the total number of rooms or bedrooms. Apparently houses with a lower bedroom/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are.

This round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step.


We can also create a geographical scatterplot of the data:

```{r point-long-lat, fig.cap="Scatterplot of longitude and latitude", out.width='80%'}

train_data %>% 
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(color = "cornflowerblue")

```

A better visualization that highlights high-density areas:

```{r point-long-lat-a, fig.cap="Scatterplot of longitude and latitude that highlights high-density areas", out.width='80%'}

train_data %>% 
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(color = "cornflowerblue", alpha = 0.1) 
  
```

Overview about California housing prices: 

- red is expensive, 
- purple is cheap and 
- larger circles indicate areas with a larger population.


```{r plot-ca-prices, fig.cap="California housing_df prices", out.width='80%'}

train_data %>% 
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(aes(size = population, color = median_house_value), 
             alpha = 0.4) +
  scale_colour_gradientn(colours=rev(rainbow(4)))

```

Lastly, we add a map to our data:

```{r}
library(ggmap)

qmplot(x = longitude, 
       y = latitude, 
       data = train_data, 
       geom = "point", 
       color = median_house_value, 
       size = population,
       alpha = 0.4) +
  scale_colour_gradientn(colours=rev(rainbow(4)))

```



# Data preparation

Prepare the data for learning algorithms.

:::note
Data preparation:

- Handle missing values
- Fix or remove outliers  
- Feature selection
- Feature engineering
- Feature scaling
:::


Next, we’ll explore the tidymodels packages `recipes` and `workflows` which are designed to help you preprocess your data before training your model. `Recipes` are built as a series of optional preprocessing steps, such as:

* Data cleaning: Fix or remove outliers, fill in missing values (e.g., with zero, mean, median…) or drop their rows (or columns).

* Feature selection: Drop the attributes that provide no useful information for the task.

* Feature engineering: Discretize continuous features, decompose features (e.g., categorical, date/time, etc.), add promising transformations of features (e.g., log(x), sqrt(x), x2 , etc.) or
aggregate features into promising new features.

* Feature scaling: Standardize or normalize features.

To combine all of the steps discussed above with model building, we can use the package [workflows](https://workflows.tidymodels.org). A workflow is an object that can bundle together your pre-processing, modeling, and post-processing requests. 


## Create recipe and roles 

To get started, let’s create a recipe for our regression model. Before training the models, we can use a recipe to create a few new predictors and conduct some preprocessing required by the model.

* The `recipe()` function has two arguments: *A formula*. Any variable on the left-hand side of the tilde (`~`) is considered the model outcome (here, `median_house_value`). On the right-hand side of the tilde are the predictors. Variables may be listed by name, or you can use the dot (`.`) to indicate all other variables as predictors. *The data*. A recipe is associated with the data set used to create the model. This will typically be the training set, so `data = train_data` here. 


* `update_role()`: This step of adding roles to a recipe is optional; the purpose of using it here is that those two variables can be retained in the data but not included in the model. This can be convenient when, after the model is fit, we want to investigate some poorly predicted value. These ID columns will be available and can be used to try to understand what went wrong. 

* `step_naomit()` creates a specification of a recipe step that will remove observations (rows of data) if they contain NA or NaN values

* `step_novel()` will convert all nominal variables to factors.

* `step_dummy()`: We then convert the factor columns into (one or more) numeric binary (0 and 1) variables for the levels of the training data.

* `step_zv()`: We remove any numeric variables that have zero variance.

* `step_normalize()`: We normalize (center and scale) the numeric variables (this is also called z-transformation). 



```{r}

housing_rec <-
  recipe(median_house_value ~ ., data = new_train) %>%
  update_role(longitude, 
              latitude, 
              new_role = "ID") %>% 
  step_naomit(all_predictors()) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>%
  step_dummy(all_nominal()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())
  
```

To get the current set of variables and roles, use the `summary()` function:

```{r}

summary(housing_rec)

```

















We can drop rows containing missing values with the function `drop_na()`:

```{r}

housing_df <- 
  housing_df %>% 
  drop_na(total_bedrooms)

```



# Model building

## Model specification

1. Pick a `model type`: choose from this [list](https://www.tidymodels.org/find/parsnip/)
2. Set the `engine`: choose from this [list](https://www.tidymodels.org/find/parsnip/)
3. Set the `mode`: regression or classification

```{r}
library(tidymodels)

lm_spec <- # your model specification
  linear_reg() %>%  # model type
  set_engine(engine = "lm") %>%  # model engine
  set_mode("regression") # model mode

# Show your model specification
lm_spec

```















## Model training

In the training process, you run an algorithm on data and thereby produce a model. This process is also called model fitting.

```{r}

lm_fit <- # your fitted model
  lm_spec %>%  # your model specification  
  fit( 
  median_house_value ~ median_income, # a Linear Regression formula 
  data = train_data # your data
  )

# Show your fitted model
lm_fit

```

## Model predictions

We use our fitted model to make predictions. 

```{r}

price_pred <- 
  lm_fit %>% 
  predict(new_data = train_data) %>%
  mutate(price_truth = train_data$median_house_value)

head(price_pred)

```











## Model evaluation 

We use the Root Mean Squared Error (RMSE) to evaluate our regression model. Therefore, we use the function $rmse(data, truth, estimate)$.

```{r}

rmse(data = price_pred, 
     truth = price_truth, 
     estimate = .pred)

```

