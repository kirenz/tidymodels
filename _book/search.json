[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"book ’ll cover tidymodels framework, collection packages modeling machine learning using tidyverse principles build regression classification models. Furthermore, follow data science lifecycle process proposed Wirth Hipp (2000):\nFigure 0.1: Cross Industry Standard Process Data Mining (Wirth Hipp (2000))\nlearn framework, review presentation CRISP-DM.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"online work licensed Creative Commons Attribution-ShareAlike 4.0 Internationale.","code":""},{"path":"index.html","id":"acknowledements","chapter":"Welcome","heading":"Acknowledements","text":"content tutorials mainly adapted excellent book “Hands-machine learning scikit-learn, keras tensorflow” Géron (2019).website built bookdown.","code":""},{"path":"crisp-dm.html","id":"crisp-dm","chapter":"1 CRISP-DM","heading":"1 CRISP-DM","text":"data science projects, follow process proposed “cross industry standard process data mining (CRISP-DM)” Wirth Hipp (2000):\nFigure 1.1: Cross Industry Standard Process Data Mining (Wirth Hipp (2000))\nlearn framework, review presentation CRISP-DM.Next, show crucial steps framework.","code":""},{"path":"crisp-dm.html","id":"business-understanding","chapter":"1 CRISP-DM","heading":"1.1 Business understanding","text":"Define (business) goalFrame problem (regression, classification,…)Choose performance measure (RMSE, …)Show data processing components (data pipeline)","code":""},{"path":"crisp-dm.html","id":"data-understanding","chapter":"1 CRISP-DM","heading":"1.2 Data understanding","text":"Import dataClean dataFormat data properly (numeric categorical)Create new variablesOverview complete dataSplit data training test set using stratified samplingDiscover visualize data gain insights (copy training data)","code":""},{"path":"crisp-dm.html","id":"data-preparation","chapter":"1 CRISP-DM","heading":"1.3 Data preparation","text":"Perform feature selection (choose predictor variables)feature engineering (mainly recipes)Create validation set training data (e.g., k-fold crossvalidation)","code":""},{"path":"crisp-dm.html","id":"modeling","chapter":"1 CRISP-DM","heading":"1.4 Modeling","text":"Specify modelsBundle data preprocessing recipe model workflowCompare model performance validation setPick model best validation setTrain best model training dataDouble-check model test set.","code":""},{"path":"intro.html","id":"intro","chapter":"2 Introduction","heading":"2 Introduction","text":"chapter, ’ll build following models:lasso,natural spline,random forest,XGBoost (extreme gradient boosted trees)","code":""},{"path":"business-understanding-1.html","id":"business-understanding-1","chapter":"3 Business understanding","heading":"3 Business understanding","text":"business understanding, :Define (business) goalFrame problem (regression, classification,…)Choose performance measureShow data processing componentsFirst , take look big picture define objective data science project business terms.example, goal build model housing prices California. particular, model learn California census data able predict median house price district (population 600 3000 people), given predictor variables. Hence, face supervised learning situation use regression model predict numerical outcomes. Furthermore, use root mean square error (RMSE) performance measure regression problem.Let’s assume model’s output (prediction district’s median housing price) fed another analytics system, along data. downstream system determine whether worth investing given area . data processing components (also called data pipeline) shown 3.1 (can use Google’s architectural templates draw data pipeline).\nFigure 3.1: Data processing components\n","code":""},{"path":"data-understanding-1.html","id":"data-understanding-1","chapter":"4 Data understanding","heading":"4 Data understanding","text":"Data Understanding, :Import dataClean dataFormat data properlyCreate new variablesGet overview complete dataSplit data training test set using stratified samplingDiscover visualize data gain insights","code":""},{"path":"data-understanding-1.html","id":"imort-data","chapter":"4 Data understanding","heading":"4.1 Imort Data","text":"First , let’s import data:","code":"\nlibrary(tidyverse)\n\nLINK <- \"https://raw.githubusercontent.com/kirenz/datasets/master/housing_unclean.csv\"\nhousing_df <- read_csv(LINK)"},{"path":"data-understanding-1.html","id":"clean-data","chapter":"4 Data understanding","heading":"4.2 Clean data","text":"get first impression data take look top 4 rows DataFrame:Notice values first row variables housing_median_ageand median_house_value. need remove strings “years” “$.” Therefore, use function str_remove_all stringr package. Since multiple wrong entries type, apply corrections rows corresponding variable:don’t cover phase data cleaning detail tutorial. However, real data science project, data cleaning usually time consuming process.","code":"\n\nhead(housing_df, 4)\n#> # A tibble: 4 x 10\n#>   longitude latitude housing_median_… total_rooms total_bedrooms population\n#>       <dbl>    <dbl> <chr>                  <dbl>          <dbl>      <dbl>\n#> 1     -122.     37.9 41.0years                880            129        322\n#> 2     -122.     37.9 21.0                    7099           1106       2401\n#> 3     -122.     37.8 52.0                    1467            190        496\n#> 4     -122.     37.8 52.0                    1274            235        558\n#> # … with 4 more variables: households <dbl>, median_income <dbl>,\n#> #   median_house_value <chr>, ocean_proximity <chr>\nlibrary(stringr)\n\nhousing_df <- \n  housing_df %>% \n  mutate(\n    housing_median_age = str_remove_all(housing_median_age, \"[years]\"),\n    median_house_value = str_remove_all(median_house_value, \"[$]\")\n  )"},{"path":"data-understanding-1.html","id":"format-data","chapter":"4 Data understanding","heading":"4.3 Format data","text":"Next, take look data structure check wether data formats correct:Numeric variables formatted integers (int) double precision floating point numbers (dbl).Numeric variables formatted integers (int) double precision floating point numbers (dbl).Categorical (nominal ordinal) variables usually formatted factors (fct) characters (chr). Especially, don’t many levels.Categorical (nominal ordinal) variables usually formatted factors (fct) characters (chr). Especially, don’t many levels.package visdat helps us explore data class structure visually:can observe numeric variables housing_media_age median_house_value declared characters (chr) instead numeric. choose format variables dbl, since values floating-point numbers.Furthermore, categorical variable ocean_proximity formatted character instead factor. Let’s take look levels variable:variable 5 levels therefore formatted factor.Note usually good idea first take care numerical variables. Afterwards, can easily convert remaining character variables factors using function across dplyr package (part tidyverse).","code":"\n\nglimpse(housing_df)\n#> Rows: 20,640\n#> Columns: 10\n#> $ longitude          <dbl> -122, -122, -122, -122, -122, -122, -122, -122, -1…\n#> $ latitude           <dbl> 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.8, 37.8, 37…\n#> $ housing_median_age <chr> \"41.0\", \"21.0\", \"52.0\", \"52.0\", \"52.0\", \"52.0\", \"5…\n#> $ total_rooms        <dbl> 880, 7099, 1467, 1274, 1627, 919, 2535, 3104, 2555…\n#> $ total_bedrooms     <dbl> 129, 1106, 190, 235, 280, 213, 489, 687, 665, 707,…\n#> $ population         <dbl> 322, 2401, 496, 558, 565, 413, 1094, 1157, 1206, 1…\n#> $ households         <dbl> 126, 1138, 177, 219, 259, 193, 514, 647, 595, 714,…\n#> $ median_income      <dbl> 8.33, 8.30, 7.26, 5.64, 3.85, 4.04, 3.66, 3.12, 2.…\n#> $ median_house_value <chr> \"452600.0\", \"358500.0\", \"352100.0\", \"341300.0\", \"3…\n#> $ ocean_proximity    <chr> \"NEAR BAY\", \"NEAR BAY\", \"NEAR BAY\", \"NEAR BAY\", \"N…\nlibrary(visdat)\n\nvis_dat(housing_df)\n\nhousing_df %>% \n  count(ocean_proximity,\n        sort = TRUE)\n#> # A tibble: 5 x 2\n#>   ocean_proximity     n\n#>   <chr>           <int>\n#> 1 <1H OCEAN        9136\n#> 2 INLAND           6551\n#> 3 NEAR OCEAN       2658\n#> 4 NEAR BAY         2290\n#> 5 ISLAND              5\n\n# convert to numeric\nhousing_df <- \n  housing_df %>% \n  mutate(\n    housing_median_age = as.numeric(housing_median_age),\n    median_house_value = as.numeric(median_house_value)\n  )\n\n# convert all remaining character variables to factors \nhousing_df <- \n  housing_df %>% \n  mutate(across(where(is.character), as.factor))"},{"path":"data-understanding-1.html","id":"missing-data","chapter":"4 Data understanding","heading":"4.4 Missing data","text":"Now let’s turn attention missing data. Missing data can viewed function vis_miss package visdat. arrange data columns missingness:alternative method obtain missing data:missing rate 0.1% (207 cases) variable total_bedroms. can cause problems algorithms. take care issue data preparation phase.","code":"\n\nvis_miss(housing_df, sort_miss = TRUE)\n\nis.na(housing_df) %>% colSums()\n#>          longitude           latitude housing_median_age        total_rooms \n#>                  0                  0                  0                  0 \n#>     total_bedrooms         population         households      median_income \n#>                207                  0                  0                  0 \n#> median_house_value    ocean_proximity \n#>                  0                  0"},{"path":"data-understanding-1.html","id":"create-new-variables","chapter":"4 Data understanding","heading":"4.5 Create new variables","text":"One important thing may want beginning data science project create new variable combinations. example:total number rooms district useful don’t know many households . really want number rooms per household.total number rooms district useful don’t know many households . really want number rooms per household.Similarly, total number bedrooms useful: probably want compare number rooms.Similarly, total number bedrooms useful: probably want compare number rooms.population per household also seems like interesting attribute combination look .population per household also seems like interesting attribute combination look .Let’s create new attributes:","code":"\n\nhousing_df <- \n  housing_df %>% \n  mutate(rooms_per_household = total_rooms/households,\n        bedrooms_per_room = total_bedrooms/total_rooms,\n        population_per_household = population/households)"},{"path":"data-understanding-1.html","id":"data-overview","chapter":"4 Data understanding","heading":"4.6 Data overview","text":"took care data problems, can obtain data summary numerical categorical attributes using function package skimr:Table 4.1: Data summaryVariable type: factorVariable type: numericWe 20640 observations 13 columns data.sd column shows standard deviation, measures dispersed values .sd column shows standard deviation, measures dispersed values .p0, p25, p50, p75 p100 columns show corresponding percentiles: percentile indicates value given percentage observations group observations fall. example, 25% districts housing_median_age lower 18, 50% lower 29 75% lower 37. often called 25th percentile (first quartile), median, 75th percentile.p0, p25, p50, p75 p100 columns show corresponding percentiles: percentile indicates value given percentage observations group observations fall. example, 25% districts housing_median_age lower 18, 50% lower 29 75% lower 37. often called 25th percentile (first quartile), median, 75th percentile.note median income attribute look like expressed US dollars (USD). Actually data scaled capped 15 (actually, 15.0001) higher median incomes, 0.5 (actually, 0.4999) lower median incomes. numbers represent roughly tens thousands dollars (e.g., 3 actually means $30,000).note median income attribute look like expressed US dollars (USD). Actually data scaled capped 15 (actually, 15.0001) higher median incomes, 0.5 (actually, 0.4999) lower median incomes. numbers represent roughly tens thousands dollars (e.g., 3 actually means $30,000).Another quick way get overview type data dealing plot histogram numerical attribute. histogram shows number instances (vertical axis) given value range (horizontal axis). can either plot one attribute time, can use ggscatmat package GGally whole dataset (shown following code example), plot histogram numerical attribute well correlation coefficients (Pearson default). just select promising variabels plot:Another option use ggpairs, even can integrate categorical variable ocean_proximity output:things might notice histograms:variables median income, housing median age median house value capped. latter may serious problem since target attribute (y label). Machine Learning algorithms may learn prices never go beyond limit. serious problem need predictions beyond 500,000. take care issue data preparation phase use districts 500,000.variables median income, housing median age median house value capped. latter may serious problem since target attribute (y label). Machine Learning algorithms may learn prices never go beyond limit. serious problem need predictions beyond 500,000. take care issue data preparation phase use districts 500,000.Note attributes different scales. take care issue later data preparation, use feature scaling (data normalization).Note attributes different scales. take care issue later data preparation, use feature scaling (data normalization).Finally, many histograms tail-heavy: extend much farther right median left. may make bit harder Machine Learning algorithms detect patterns. transform attributes later bell-shaped distributions. right-skewed data (.e., tail right, also called positive skew), common transformations include square root log (use log).Finally, many histograms tail-heavy: extend much farther right median left. may make bit harder Machine Learning algorithms detect patterns. transform attributes later bell-shaped distributions. right-skewed data (.e., tail right, also called positive skew), common transformations include square root log (use log).","code":"\n\nskim(housing_df)\nlibrary(GGally)\n\nhousing_df %>% \n  select(\n    median_house_value, housing_median_age, \n    median_income, bedrooms_per_room, rooms_per_household, \n    population_per_household) %>% \n  ggscatmat(alpha = 0.2)\nlibrary(GGally)\n\nhousing_df %>% \n  select(\n    median_house_value, housing_median_age, \n    median_income, bedrooms_per_room, rooms_per_household, \n    population_per_household,\n    ocean_proximity) %>% \n  ggpairs()"},{"path":"data-understanding-1.html","id":"data-splitting","chapter":"4 Data understanding","heading":"4.7 Data splitting","text":"get started -depth data exploration, let’s split single dataset two: training set testing set. training data used fit models, testing set used measure model performance. perform data exploration training data.training dataset dataset examples used learning process used fit models. test dataset dataset independent training dataset used evaluate performance final model. model fit training dataset also fits test dataset well, minimal overfitting taken place. better fitting training dataset opposed test dataset usually points overfitting.data split, want ensure training test set representative various categories median house values whole dataset. Take look 4.1\nFigure 4.1: Histogram Median Proces\ngeneral, like instances stratum, else estimate stratum’s importance may biased. stratum (plural strata) refers subset (part) whole data sampled. many strata, stratum large enough. use 4 strata example.actually split data, can use rsample package (included tidymodels) create object contains information split data (call data_split), two rsample functions create data frames training testing sets:","code":"\n\nhousing_df %>% \n  ggplot(aes(median_house_value)) +\n  geom_histogram(bins = 4) \n\n# Fix the random numbers by setting the seed \n# This enables the analysis to be reproducible \nset.seed(123)\n\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(housing_df, \n                           prop = 3/4, \n                           strata = median_house_value, \n                           breaks = 4)\n\n# Create dataframes for the two sets:\ntrain_data <- training(data_split) \ntest_data <- testing(data_split)"},{"path":"data-understanding-1.html","id":"data-exploration","chapter":"4 Data understanding","heading":"4.8 Data exploration","text":"point data exploration gain insights help select important variables model get ideas feature engineering data preparation phase. Ususally, data exploration iterative process: get prototype model running, can analyze output gain insights come back exploration step. important note perform data exploration training data.","code":""},{"path":"data-understanding-1.html","id":"create-data-copy","chapter":"4 Data understanding","heading":"4.8.1 Create data copy","text":"first make copy training data since don’t want alter data data exploration.Next, take closer look relationships variables. particular, interested relationships dependent variable median_house_value variables. goal identify possible predictor variables use models predict median_house_value.","code":"\n\ndata_explore <- train_data"},{"path":"data-understanding-1.html","id":"geographical-overview","chapter":"4 Data understanding","heading":"4.8.2 Geographical overview","text":"Since data includes information longitude latitude, start data exploration creation geographical scatterplot data get first insights:\nFigure 4.2: Scatterplot longitude latitude\nbetter visualization highlights high-density areas (parameter alpha = 0.1 ):\nFigure 4.3: Scatterplot longitude latitude highlights high-density areas\nOverview California housing prices:red expensive,purple cheap andlarger circles indicate areas larger population.\nFigure 4.4: California housing_df prices\nLastly, add map data:image tells housing prices much related location (e.g., close ocean) population density. Hence ocean_proximity variable may useful predictor median housing prices, although Northern California housing prices coastal districts high, simple rule.","code":"\n\ndata_explore %>% \n  ggplot(aes(x = longitude, y = latitude)) +\n  geom_point(color = \"cornflowerblue\")\n\ndata_explore %>% \n  ggplot(aes(x = longitude, y = latitude)) +\n  geom_point(color = \"cornflowerblue\", alpha = 0.1) \n  \n\ndata_explore %>% \n  ggplot(aes(x = longitude, y = latitude)) +\n  geom_point(aes(size = population, color = median_house_value), \n             alpha = 0.4) +\n  scale_colour_gradientn(colours=rev(rainbow(4)))\nlibrary(ggmap)\n\nqmplot(x = longitude, \n       y = latitude, \n       data = data_explore, \n       geom = \"point\", \n       color = median_house_value, \n       size = population,\n       alpha = 0.4) +\n  scale_colour_gradientn(colours=rev(rainbow(4))) +\n  scale_alpha(guide = 'none') # don't show legend for alpha"},{"path":"data-understanding-1.html","id":"boxplots","chapter":"4 Data understanding","heading":"4.8.3 Boxplots","text":"can use boxplots check, actually find differences median house value different levels categorical variable ocean_proximity. Additionally, use package ggsignif calculate significance difference two groups add annotation plot single line.can observe difference median house value different levels categorical variable (except “NEAR BAY” “NEAR OCEAN”) include variable model. Furthermore, difference “<1H OCEAN” “INLAND” statistically significant.","code":"\nlibrary(ggsignif)\n\ndata_explore %>% \n  ggplot(aes(ocean_proximity, median_house_value)) +\n  geom_boxplot(fill=\"steelblue\") +\n  xlab(\"Ocean proximity\") +\n  ylab(\"Median house value\") +\n  geom_signif(comparisons = list(c(\"<1H OCEAN\", \"INLAND\")), # calculate significance\n               map_signif_level=TRUE) \n  "},{"path":"data-understanding-1.html","id":"correlations","chapter":"4 Data understanding","heading":"4.8.4 Correlations","text":"Now let’s analyze numerical variables: obtain correlations numerical data, can use function vis_cor visdat package. use Spearman’s correlation coefficient since measure insensitive outliers Pearson’s correlation coefficient:Now take closer look correlation coefficients package corrr.Furthermore, function network_plot outputs nice network plot data whichvariables highly correlated appear closer together joined stronger paths.Paths also colored sign (blue positive red negative).proximity points determined using clusteringSummary findings correlation analysis:median_income strong positive correlation median_house_value.median_income strong positive correlation median_house_value.new bedrooms_per_room attribute negatively correlated median_house_value. Apparently houses lower bedroom/room ratio tend expensive.new bedrooms_per_room attribute negatively correlated median_house_value. Apparently houses lower bedroom/room ratio tend expensive.rooms_per_household also bit informative total number rooms (total_rooms) district. Obviously larger houses, expensive (positive correlation).rooms_per_household also bit informative total number rooms (total_rooms) district. Obviously larger houses, expensive (positive correlation).population_per_household negatively correlated dependent variable.population_per_household negatively correlated dependent variable.last step correlation analysis, check statistical significance Spearman’s rank correlations. example, obtain significant p-values:Consequently use four numerical variables well ocean_proximity predictors model.","code":"\nlibrary(visdat)\n\ndata_explore %>% \n  select(where(is.numeric)) %>% # only select numerical data\n  vis_cor(cor_method = \"spearman\", na_action = \"pairwise.complete.obs\")\nlibrary(corrr)\n\n# calculate all correlations\ncor_res <- \n  data_explore %>%\n  select(where(is.numeric)) %>% \n  correlate(method = \"spearman\", use = \"pairwise.complete.obs\") \n\n# show correlations\ncor_res %>% \n  select(term, median_house_value) %>% \n  filter(!is.na(median_house_value)) %>% # focus on dependent variable \n  arrange(median_house_value) %>% # sort values\n  fashion() # print tidy correlations\n#>                        term median_house_value\n#> 1         bedrooms_per_room               -.33\n#> 2  population_per_household               -.26\n#> 3                  latitude               -.16\n#> 4                 longitude               -.07\n#> 5                population               -.00\n#> 6        housing_median_age                .08\n#> 7            total_bedrooms                .08\n#> 8                households                .11\n#> 9               total_rooms                .20\n#> 10      rooms_per_household                .27\n#> 11            median_income                .68\n\ndata_explore %>%\n  select(where(is.numeric)) %>%  \n  correlate() %>% \n  network_plot(min_cor = .15)\n\ncor.test(data_explore$median_house_value, \n         data_explore$population_per_household, \n         method = \"spearman\", \n         exact=FALSE)$p.value\n#> [1] 1.71e-245\n\ncor.test(data_explore$median_house_value, \n         data_explore$bedrooms_per_room, \n         method = \"spearman\", \n         exact=FALSE)$p.value\n#> [1] 0\n\ncor.test(data_explore$median_house_value, \n         data_explore$rooms_per_household, \n         method = \"spearman\", \n         exact=FALSE)$p.value\n#> [1] 1.58e-247\n\ncor.test(data_explore$median_house_value, \n         data_explore$population_per_household, \n         method = \"spearman\", \n         exact=FALSE)$p.value\n#> [1] 1.71e-245"},{"path":"data-understanding-1.html","id":"visual-inspections","chapter":"4 Data understanding","heading":"4.8.5 Visual inspections","text":"Now let’s analyze choosen variables detail. function ggscatmat package GGally creates matrix scatterplots, densities correlations numeric columns. code choose alpha level 0.2 (transparency).can also add color column categorical variable ocean_proximity get even insights :can observe ocean proximity variable indeed good predictor different median house values. Another promising attribute predict median house value median income, let’s zoom :plot reveals things. First, correlation indeed strong; can clearly see upward trend, points dispersed. Second, price cap noticed earlier clearly visible horizontal line 500,000 dollars. plot reveals less obvious straight lines: horizontal line around 450,000 dollars, another around 350,000 dollars, perhaps one around $280,000 dollars, . Hence, data preparation phase remove districts 500,000 dollars prevent algorithms learning reproduce data quirks.","code":"\n\ndata_explore %>% \n  select(median_house_value, ocean_proximity, \n         median_income, bedrooms_per_room, rooms_per_household, \n         population_per_household) %>% \n  ggscatmat(corMethod = \"spearman\",\n            alpha=0.2)\n\ndata_explore %>% \n  select(median_house_value, ocean_proximity, \n         median_income, bedrooms_per_room, rooms_per_household, \n         population_per_household) %>% \n  ggscatmat(color=\"ocean_proximity\", # add a categorical variable\n            corMethod = \"spearman\",\n            alpha=0.2)\n\ndata_explore %>% \n  ggplot(aes(median_income, median_house_value)) +\n  geom_jitter(color = \"steelblue\", alpha = 0.2) + \n  xlab(\"Median income\") +\n  ylab(\"Median house value\") +\n  scale_y_continuous(labels = scales::dollar)"},{"path":"data-preparation-1.html","id":"data-preparation-1","chapter":"5 Data preparation","heading":"5 Data preparation","text":"Data preparation:Handle missing valuesFix remove outliersFeature selectionFeature engineeringFeature scalingCreate validation setNext, ’ll preprocess data training models. mainly use tidymodels packages recipes workflows steps. Recipes built series optional data preparation steps, :Data cleaning: Fix remove outliers, fill missing values (e.g., zero, mean, median…) drop rows (columns).Data cleaning: Fix remove outliers, fill missing values (e.g., zero, mean, median…) drop rows (columns).Feature selection: Drop attributes provide useful information task.Feature selection: Drop attributes provide useful information task.Feature engineering: Discretize continuous features, decompose features (e.g., weekday date variable, etc.), add promising transformations features (e.g., log(x), sqrt(x), x2 , etc.) aggregate features promising new features (like already ).Feature engineering: Discretize continuous features, decompose features (e.g., weekday date variable, etc.), add promising transformations features (e.g., log(x), sqrt(x), x2 , etc.) aggregate features promising new features (like already ).Feature scaling: Standardize normalize features.Feature scaling: Standardize normalize features.want use recipe across several steps train test models. simplify process, can use model workflow, pairs model recipe together.","code":""},{"path":"data-preparation-1.html","id":"data-preparation-2","chapter":"5 Data preparation","heading":"5.1 Data preparation","text":"create recipes, first select variables use model. also remove specific cases price median house value equal greater 500000 dollars. Note keep longitude latitude able map data later stage use variables model.Furthermore, need make new data split since updated original data.","code":"\n\nhousing_df_new <-\n  housing_df %>% \n  filter(median_house_value < 500000) %>% # only use houses with a value below 500000\n  select( # select our predictors\n    longitude, latitude, \n    median_house_value, \n    median_income, \n    ocean_proximity, \n    bedrooms_per_room, \n    rooms_per_household, \n    population_per_household\n         )\n\nglimpse(housing_df_new)\n#> Rows: 19,648\n#> Columns: 8\n#> $ longitude                <dbl> -122, -122, -122, -122, -122, -122, -122, -1…\n#> $ latitude                 <dbl> 37.9, 37.9, 37.9, 37.9, 37.9, 37.9, 37.8, 37…\n#> $ median_house_value       <dbl> 452600, 358500, 352100, 341300, 342200, 2697…\n#> $ median_income            <dbl> 8.33, 8.30, 7.26, 5.64, 3.85, 4.04, 3.66, 3.…\n#> $ ocean_proximity          <fct> NEAR BAY, NEAR BAY, NEAR BAY, NEAR BAY, NEAR…\n#> $ bedrooms_per_room        <dbl> 0.147, 0.156, 0.130, 0.184, 0.172, 0.232, 0.…\n#> $ rooms_per_household      <dbl> 6.98, 6.24, 8.29, 5.82, 6.28, 4.76, 4.93, 4.…\n#> $ population_per_household <dbl> 2.56, 2.11, 2.80, 2.55, 2.18, 2.14, 2.13, 1.…\nset.seed(123)\n\ndata_split <- initial_split(housing_df_new, # updated data\n                           prop = 3/4, \n                           strata = median_house_value, \n                           breaks = 4)\n\ntrain_data <- training(data_split) \ntest_data <- testing(data_split)"},{"path":"data-preparation-1.html","id":"data-prepropecessing-recipe","chapter":"5 Data preparation","heading":"5.2 Data prepropecessing recipe","text":"type data preprocessing dependent data type model fit. excellent book “Tidy Modeling R” provides appendix recommendations baseline levels preprocessing needed various model functions (Kuhn Silge (2021))Let’s create base recipe regression models (one models, need add new recipe step later stage). Note sequence steps matter:recipe() function two arguments:recipe() function two arguments:formula. variable left-hand side tilde (~) considered model outcome (, median_house_value). right-hand side tilde predictors. Variables may listed name (separated +), can use dot (.) indicate variables predictors.formula. variable left-hand side tilde (~) considered model outcome (, median_house_value). right-hand side tilde predictors. Variables may listed name (separated +), can use dot (.) indicate variables predictors.data. recipe associated data set used create model. typically training set, data = train_data .data. recipe associated data set used create model. typically training set, data = train_data .update_role(): step adding roles recipe optional; purpose using two variables can retained data included model. can convenient , model fit, want investigate poorly predicted value. ID columns available can used try understand went wrong.update_role(): step adding roles recipe optional; purpose using two variables can retained data included model. can convenient , model fit, want investigate poorly predicted value. ID columns available can used try understand went wrong.step_naomit() removes observations (rows data) contain NA NaN values. use skip = TRUE don’t want perform part new data number samples assessment set number predicted values (even NA).step_naomit() removes observations (rows data) contain NA NaN values. use skip = TRUE don’t want perform part new data number samples assessment set number predicted values (even NA).step_novel() converts nominal variables factors takes care issues related categorical variables.step_novel() converts nominal variables factors takes care issues related categorical variables.step_log() log transform data (since numerical variables right-skewed). Note step can performed negative numbers.step_log() log transform data (since numerical variables right-skewed). Note step can performed negative numbers.step_normalize() normalizes (center scales) numeric variables (.e., z-standardization).step_normalize() normalizes (center scales) numeric variables (.e., z-standardization).step_dummy() converts factor column ocean_proximity numeric binary (0 1) variables.step_dummy() converts factor column ocean_proximity numeric binary (0 1) variables.step_zv(): removes numeric variables zero variance.step_zv(): removes numeric variables zero variance.step_corr(): remove predictor variables large correlations predictor variables.step_corr(): remove predictor variables large correlations predictor variables.view current set variables roles, use summary() function:like check preprocessing steps actually worked, can proceed follows:Take look data structure:Visualize data:notice :variables longitude latitude change.variables longitude latitude change.median_income, rooms_per_household population_per_household now z-standardized distributions bit less right skewed (due log transformation)median_income, rooms_per_household population_per_household now z-standardized distributions bit less right skewed (due log transformation)ocean_proximity replaced dummy variables.ocean_proximity replaced dummy variables.","code":"\n\nhousing_rec <-\n  recipe(median_house_value ~ .,\n         data = train_data) %>%\n  update_role(longitude, latitude, \n              new_role = \"ID\") %>% \n  step_log(\n    median_house_value, median_income,\n    bedrooms_per_room, rooms_per_household, \n    population_per_household\n    ) %>% \n  step_naomit(everything(), skip = TRUE) %>% \n  step_novel(all_nominal(), -all_outcomes()) %>%\n  step_normalize(all_numeric(), -all_outcomes(), \n                 -longitude, -latitude) %>% \n  step_dummy(all_nominal()) %>%\n  step_zv(all_numeric(), -all_outcomes()) %>%\n  step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") \n\nsummary(housing_rec)\n#> # A tibble: 8 x 4\n#>   variable            type    role      source  \n#>   <chr>               <chr>   <chr>     <chr>   \n#> 1 longitude           numeric ID        original\n#> 2 latitude            numeric ID        original\n#> 3 median_income       numeric predictor original\n#> 4 ocean_proximity     nominal predictor original\n#> 5 bedrooms_per_room   numeric predictor original\n#> 6 rooms_per_household numeric predictor original\n#> # … with 2 more rows\n\nprepped_data <- \n  housing_rec %>% # use the recipe object\n  prep() %>% # perform the recipe on training data\n  juice() # extract only the preprocessed dataframe \n\nglimpse(prepped_data)\n#> Rows: 14,582\n#> Columns: 10\n#> $ longitude                  <dbl> -122, -122, -122, -122, -122, -122, -122, …\n#> $ latitude                   <dbl> 37.9, 37.9, 37.9, 37.9, 37.9, 37.8, 37.8, …\n#> $ median_income              <dbl> 2.0488, 1.7451, 1.1765, 0.3102, 0.4195, 0.…\n#> $ rooms_per_household        <dbl> 0.7212, 1.7805, 0.4608, 0.7472, -0.2857, -…\n#> $ population_per_household   <dbl> -1.1528, -0.0806, -0.4400, -1.0267, -1.099…\n#> $ median_house_value         <dbl> 12.8, 12.8, 12.7, 12.7, 12.5, 12.6, 12.4, …\n#> $ ocean_proximity_INLAND     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ ocean_proximity_ISLAND     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ ocean_proximity_NEAR.BAY   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#> $ ocean_proximity_NEAR.OCEAN <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\nprepped_data %>% \n  select(median_house_value, \n         median_income, \n         rooms_per_household, \n         population_per_household) %>% \n  ggscatmat(corMethod = \"spearman\",\n            alpha=0.2)"},{"path":"data-preparation-1.html","id":"validation-set","chapter":"5 Data preparation","heading":"5.3 Validation set","text":"Remember already partitioned data set training set test set. lets us judge whether given model generalize well new data. However, using two partitions may insufficient many rounds hyperparameter tuning (don’t perform tutorial always recommended use validation set).Therefore, usually good idea create called validation set. Watch short video Google’s Machine Learning crash course learn value validation set.use k-fold crossvalidation build set 5 validation folds function vfold_cv. also use stratified sampling:come back validation set specified models.","code":"\n\nset.seed(100)\n\ncv_folds <-\n vfold_cv(train_data, \n          v = 5, # number of folds\n          strata = median_house_value,\n          breaks = 4) "},{"path":"model-building.html","id":"model-building","chapter":"6 Model building","heading":"6 Model building","text":"","code":""},{"path":"model-building.html","id":"specify-models","chapter":"6 Model building","heading":"6.1 Specify models","text":"process specifying models always follows:Pick model typeset engineSet mode: regression classificationYou can choose model type engine list.","code":""},{"path":"model-building.html","id":"lasso-regression","chapter":"6 Model building","heading":"6.1.1 Lasso regression","text":"penalty: total amount regularization model. Higher values imply higher penalty. choose penalty 0 fit standard linear regression model.penalty: total amount regularization model. Higher values imply higher penalty. choose penalty 0 fit standard linear regression model.mixture: mixture amounts different types regularization. number zero one (inclusive) proportion L1 regularization (.e. lasso) model. mixture = 1, pure lasso model mixture = 0 indicates ridge regression used (works engines “glmnet” “spark”).mixture: mixture amounts different types regularization. number zero one (inclusive) proportion L1 regularization (.e. lasso) model. mixture = 1, pure lasso model mixture = 0 indicates ridge regression used (works engines “glmnet” “spark”).Note lasso regression work properly important always add data normalization step.","code":"\n\nlasso_spec <- # your model specification\n  linear_reg(penalty = 0.1, mixture = 1) %>%  # model type and some options\n  set_engine(engine = \"glmnet\") %>%  # model engine\n  set_mode(\"regression\") # model mode\n\n# Show your model specification\nlasso_spec\n#> Linear Regression Model Specification (regression)\n#> \n#> Main Arguments:\n#>   penalty = 0.1\n#>   mixture = 1\n#> \n#> Computational engine: glmnet"},{"path":"model-building.html","id":"natural-spline","chapter":"6 Model building","heading":"6.1.2 Natural spline","text":"use model correctly, also need add data normalization step well step declare degree freedom model. include degrees freedom later step (create workflows).","code":"\n\nspline_spec <- \n  linear_reg() %>%  \n  set_engine(engine = \"lm\") %>%  \n  set_mode(\"regression\")"},{"path":"model-building.html","id":"random-forest","chapter":"6 Model building","heading":"6.1.3 Random forest","text":"","code":"\nlibrary(ranger)\n\nrf_spec <- \n  rand_forest() %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"regression\")"},{"path":"model-building.html","id":"boosted-tree-xgboost","chapter":"6 Model building","heading":"6.1.4 Boosted tree (XGBoost)","text":"","code":"\nlibrary(xgboost)\n\nxgb_spec <- \n  boost_tree() %>% \n  set_engine(\"xgboost\") %>% \n  set_mode(\"regression\") "},{"path":"model-building.html","id":"k-nearest-neighbor","chapter":"6 Model building","heading":"6.1.5 K-nearest neighbor","text":"","code":"\n\nknn_spec <- \n  nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors \n  set_engine(\"kknn\") %>% \n  set_mode(\"regression\") "},{"path":"model-building.html","id":"create-workflows","chapter":"6 Model building","heading":"6.2 Create workflows","text":"combine data preparation recipe model building, use package workflows. workflow object can bundle together pre-processing recipe, modeling, even post-processing requests (like calculating RMSE).","code":""},{"path":"model-building.html","id":"lasso","chapter":"6 Model building","heading":"6.2.1 Lasso","text":"Bundle recipe model workflows:","code":"\n\nlasso_wflow <- # new workflow object\n workflow() %>% # use workflow function\n add_recipe(housing_rec) %>%   # use the new recipe\n add_model(lasso_spec)   # add your model spec"},{"path":"model-building.html","id":"natural-spline-1","chapter":"6 Model building","heading":"6.2.2 Natural spline","text":"need declare degrees freedom -step_ns()- natural spline. example, just add new step housing_rec recipe create new recipe use ourse natural spline.higher degree freedom, complex resulting model.Now bundle recipe model:","code":"\nlibrary(splines)\n\nhousing_rec_spline <- \n  housing_rec %>%  \n  step_ns(all_predictors(), deg_free = 3) # natural spline\n\nspline_wflow <- \n workflow() %>% \n add_recipe(housing_rec_spline) %>%   # use the spline recipe\n add_model(spline_spec) "},{"path":"model-building.html","id":"random-forest-1","chapter":"6 Model building","heading":"6.2.3 Random forest","text":"Bundle recipe model:","code":"\n\nrf_wflow <-\n workflow() %>%\n add_recipe(housing_rec) %>% \n add_model(rf_spec) "},{"path":"model-building.html","id":"xgboost","chapter":"6 Model building","heading":"6.2.4 XGBoost","text":"Bundle recipe model:","code":"\n\nxgb_wflow <-\n workflow() %>%\n add_recipe(housing_rec) %>% \n add_model(xgb_spec)"},{"path":"model-building.html","id":"k-nearest-neighbor-1","chapter":"6 Model building","heading":"6.2.5 K-nearest neighbor","text":"Bundle recipe model:","code":"\n\nknn_wflow <-\n workflow() %>%\n add_recipe(housing_rec) %>% \n add_model(knn_spec)"},{"path":"model-building.html","id":"evaluate-models","chapter":"6 Model building","heading":"6.3 Evaluate models","text":"Now can use validation set (cv_folds) estimate performance models using fit_resamples() function fit models folds store results.Note fit_resamples() fit model resample evaluate heldout set resample. function usually used computing performance metrics across set resamples evaluate models (like RMSE) - models even stored. However, example save predictions order visualize model fit residuals control_resamples(save_pred = TRUE).Finally, collect performance metrics collect_metrics() pick model best validation set.","code":""},{"path":"model-building.html","id":"lasso-regression-1","chapter":"6 Model building","heading":"6.3.1 Lasso regression","text":"Show average performance folds:Show performance every single fold:assess model predictions, plot predictions y-axis real median house value x-axis. Note red line model. model made mistakes , points lie red diagonal line (prediction equals real value).Let`s look 10 districts model produced greatest residuals:Show observations training data.tutorial, don’t investigate reasons wrong predictions. reality, check wether districts outliers comparision rest data need decide drop cases data (good reasons ).","code":"\n\nset.seed(100)\n\nlasso_res <- \n  lasso_wflow %>% # use workflow object\n  fit_resamples(resamples = cv_folds,\n                control = control_resamples(save_pred = TRUE) # svae predictions\n    )\n\nlasso_res %>%  collect_metrics(summarize = TRUE)\n#> # A tibble: 2 x 6\n#>   .metric .estimator  mean     n std_err .config             \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 rmse    standard   0.357     5 0.00119 Preprocessor1_Model1\n#> 2 rsq     standard   0.608     5 0.00356 Preprocessor1_Model1\n\nlasso_res %>%  collect_metrics(summarize = FALSE)\n#> # A tibble: 10 x 5\n#>   id    .metric .estimator .estimate .config             \n#>   <chr> <chr>   <chr>          <dbl> <chr>               \n#> 1 Fold1 rmse    standard       0.356 Preprocessor1_Model1\n#> 2 Fold1 rsq     standard       0.599 Preprocessor1_Model1\n#> 3 Fold2 rmse    standard       0.354 Preprocessor1_Model1\n#> 4 Fold2 rsq     standard       0.613 Preprocessor1_Model1\n#> 5 Fold3 rmse    standard       0.361 Preprocessor1_Model1\n#> 6 Fold3 rsq     standard       0.608 Preprocessor1_Model1\n#> # … with 4 more rows\n\nassess_res <- collect_predictions(lasso_res)\n\nassess_res %>% \n  ggplot(aes(x = median_house_value, y = .pred)) + \n  geom_point(alpha = .15) +\n  geom_abline(col = \"red\") + \n  coord_obs_pred() + \n  ylab(\"Predicted\")\n\nwrongest_prediction <- \n  assess_res %>% \n  mutate(residual = median_house_value - .pred) %>% \n  arrange(desc(abs(residual))) %>% \n  slice_head(n = 10)\n\nwrongest_prediction\n#> # A tibble: 10 x 6\n#>   id    .pred  .row median_house_value .config              residual\n#>   <chr> <dbl> <int>              <dbl> <chr>                   <dbl>\n#> 1 Fold1  11.9  6523               9.62 Preprocessor1_Model1    -2.30\n#> 2 Fold3  12.0  4197               9.77 Preprocessor1_Model1    -2.21\n#> 3 Fold3  12.0  1348              10.0  Preprocessor1_Model1    -2.01\n#> 4 Fold5  11.6  2084               9.62 Preprocessor1_Model1    -1.98\n#> 5 Fold5  11.5  1868               9.62 Preprocessor1_Model1    -1.87\n#> 6 Fold1  12.0 12383              10.3  Preprocessor1_Model1    -1.79\n#> # … with 4 more rows\n\ntrain_data %>% \n  dplyr::slice(wrongest_prediction$.row) \n#> # A tibble: 10 x 8\n#>   longitude latitude median_house_va… median_income ocean_proximity\n#>       <dbl>    <dbl>            <dbl>         <dbl> <fct>          \n#> 1     -118.     34.2            14999          4.19 INLAND         \n#> 2     -118.     34.2            17500          2.37 <1H OCEAN      \n#> 3     -122.     37.9            22500          2.68 NEAR BAY       \n#> 4     -117.     36.4            14999          2.1  INLAND         \n#> 5     -123.     39.7            14999          1.66 INLAND         \n#> 6     -121.     34.7            28300          2.74 NEAR OCEAN     \n#> # … with 4 more rows, and 3 more variables: bedrooms_per_room <dbl>,\n#> #   rooms_per_household <dbl>, population_per_household <dbl>"},{"path":"model-building.html","id":"natural-spline-2","chapter":"6 Model building","heading":"6.3.2 Natural spline","text":"don’t repeat steps shown lasso regression just focus performance metrics.","code":"\n\nspline_res <-\n  spline_wflow %>% \n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n    )\n\nspline_res %>%  collect_metrics(summarize = TRUE)\n#> # A tibble: 2 x 6\n#>   .metric .estimator  mean     n std_err .config             \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 rmse    standard   0.308     4 0.00101 Preprocessor1_Model1\n#> 2 rsq     standard   0.665     4 0.00199 Preprocessor1_Model1"},{"path":"model-building.html","id":"random-forest-2","chapter":"6 Model building","heading":"6.3.3 Random forest","text":"don’t repeat steps shown lasso regression just focus performance metrics.","code":"\n\nrf_res <-\n  rf_wflow %>% \n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n    )\n\nrf_res %>%  collect_metrics(summarize = TRUE)\n#> # A tibble: 2 x 6\n#>   .metric .estimator  mean     n std_err .config             \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 rmse    standard   0.309     5 0.00135 Preprocessor1_Model1\n#> 2 rsq     standard   0.673     5 0.00144 Preprocessor1_Model1"},{"path":"model-building.html","id":"xgboost-1","chapter":"6 Model building","heading":"6.3.4 XGBoost","text":"don’t repeat steps shown lasso regression just focus performance metrics.","code":"\n\nxgb_res <- \n  xgb_wflow %>% \n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n    ) \n\nxgb_res %>% collect_metrics(summarize = TRUE)\n#> # A tibble: 2 x 6\n#>   .metric .estimator  mean     n std_err .config             \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 rmse    standard   0.304     5 0.00158 Preprocessor1_Model1\n#> 2 rsq     standard   0.685     5 0.00102 Preprocessor1_Model1"},{"path":"model-building.html","id":"k-nearest-neighbor-2","chapter":"6 Model building","heading":"6.3.5 K-nearest neighbor","text":"don’t repeat steps shown lasso regression just focus performance metrics.","code":"\n\nknn_res <- \n  knn_wflow %>% \n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n    ) \n\nknn_res %>% collect_metrics(summarize = TRUE)\n#> # A tibble: 2 x 6\n#>   .metric .estimator  mean     n std_err .config             \n#>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 rmse    standard   0.338     5 0.00136 Preprocessor1_Model1\n#> 2 rsq     standard   0.611     5 0.00184 Preprocessor1_Model1"},{"path":"model-building.html","id":"compare-models","chapter":"6 Model building","heading":"6.3.6 Compare models","text":"Extract RMSE models compare :Note model results quite similar.Now ’s time fit best model (case XGBoost model) one last time full training set evaluate resulting final model test set.","code":"\n\nlasso_rmse <- \n  lasso_res %>% \n  collect_metrics(summarise = TRUE) %>%\n  mutate(model = \"lasso\")\n\nspline_rmse <- \n  spline_res %>% \n  collect_metrics(summarise = TRUE) %>%\n  mutate(model = \"spline\")\n\nrf_rmse <- \n  rf_res %>% \n  collect_metrics(summarise = TRUE) %>%\n  mutate(model = \"random forest\")\n\nxgb_rmse <- \n  xgb_res %>% \n  collect_metrics(summarise = TRUE) %>%\n  mutate(model = \"XGBoost\")\n\nknn_rmse <- \n  knn_res %>% \n  collect_metrics(summarise = TRUE) %>%\n  mutate(model = \"Knn\")\n\n# create dataframe with all models\nmodel_compare <- bind_rows(lasso_rmse,\n                           spline_rmse,\n                           rf_rmse,\n                           xgb_rmse,\n                           knn_rmse) \n\n# change data structure\nmodel_comp <- \n  model_compare %>% \n  select(model, .metric, mean, std_err) %>% \n  pivot_wider(names_from = .metric, values_from = c(mean, std_err)) \n\n# show rmse \nmodel_comp %>% \n  arrange(mean_rmse) %>% \n  mutate(model = fct_reorder(model, mean_rmse)) %>%\n  ggplot(aes(model, mean_rmse, fill=model)) +\n  geom_col() +\n  scale_fill_brewer(palette = \"Blues\")\n\n# show rsq \nmodel_comp %>% \n  arrange(mean_rsq) %>% \n  mutate(model = fct_reorder(model, desc(mean_rsq))) %>%\n  ggplot(aes(model, mean_rsq, fill=model)) +\n  geom_col() +\n  scale_fill_brewer(palette = \"Blues\")\n  \n# find minimum rmse\nmodel_comp %>% \n  slice_min(mean_rmse)\n#> # A tibble: 1 x 5\n#>   model   mean_rmse mean_rsq std_err_rmse std_err_rsq\n#>   <chr>       <dbl>    <dbl>        <dbl>       <dbl>\n#> 1 XGBoost     0.304    0.685      0.00158     0.00102"},{"path":"last-fit-and-evaluation-on-test-set.html","id":"last-fit-and-evaluation-on-test-set","chapter":"7 Last fit and evaluation on test set","heading":"7 Last fit and evaluation on test set","text":"Tidymodels provides function last_fit() fits model training data evaluates test set. just need provide workflow object best model well data split object (training data).final result. Remember model fit training dataset also fits test dataset well, minimal overfitting taken place. seems also case example.model result good enogh used project can decided people expertise domain housing prices.","code":"\n\nlast_fit_xgb <- last_fit(xgb_wflow, split = data_split)\n\n# Show RMSE and RSQ\nlast_fit_xgb %>% \n  collect_metrics()\n#> # A tibble: 2 x 4\n#>   .metric .estimator .estimate .config             \n#>   <chr>   <chr>          <dbl> <chr>               \n#> 1 rmse    standard       0.305 Preprocessor1_Model1\n#> 2 rsq     standard       0.682 Preprocessor1_Model1"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
